# 20240211

# title 

# chinese 
1. 模型架构，只要足够深，到了一定的深度，“Bigness is the Betterness",简单说就是大力出奇迹，算力加数据，越大越好；
2. 任何范式都需要一个引擎，这个引擎能够不断被改进和产生价值，如果说内燃机是工业革命范式的动力引擎，现在这个引擎就是Transformer；
3. 如果你能够高效的压缩信息，你就已经得到了知识，不然你没法压缩信息。"
伊尔亚·苏茨克维与英伟达首席执行官黄仁勋在 GTC（GPU Technology Conference）活动上有一个对谈——“AI Today and Vision of the future中提到他的三个信念：
1. 模型架构，只要足够深，到了一定的深度，“Bigness is the Betterness",简单说就是大力出奇迹，算力加数据，越大越好；
2. 任何范式都需要一个引擎，这个引擎能够不断被改进和产生价值，如果说内燃机是工业革命范式的动力引擎，现在这个引擎就是Transformer；
3. 如果你能够高效的压缩信息，你就已经得到了知识，不然你没法压缩信息。"

# english
"Ilya Sutskever and NVIDIA CEO Jensen Huang had a chat at the GPU Technology Conference (GTC) about 'AI Today and the Vision of the Future.' Ilya shared three core beliefs:

First, when it comes to model architecture, size matters. The deeper the model, the better it performs. 'Bigness is the Betterness' basically means that with enough computing power and data, bigger is definitely better.

Second, every paradigm shift needs an engine to drive it. Just as the internal combustion engine was the powerhouse of the industrial revolution, today's powerhouse is the Transformer model. It's the engine that keeps improving and generating value.

And third, if you can compress information efficiently, you've essentially captured knowledge. If you can't compress information, you don't truly understand it."
