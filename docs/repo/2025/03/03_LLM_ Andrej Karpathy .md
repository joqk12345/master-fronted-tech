



# æ–‡æ¡£æ ‡é¢˜ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å®ç”¨æŒ‡å—ä¸é«˜çº§åº”ç”¨

## å¼•è¨€

æœ¬ç®€æŠ¥æ—¨åœ¨ä»‹ç»å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å®é™…åº”ç”¨ï¼Œå¹¶é€šè¿‡å…·ä½“æ¡ˆä¾‹è®²è§£å¦‚ä½•æœ‰æ•ˆåˆ©ç”¨è¿™äº›å·¥å…·æ¥æå‡å·¥ä½œæ•ˆç‡å’Œè§£å†³å®é™…é—®é¢˜ã€‚ æœ¬æ–‡ä»¶åŸºäºå®é™…æ“ä½œæ¼”ç¤ºå’Œç»éªŒæ€»ç»“ï¼Œæ—¨åœ¨ä¸ºè¯»è€…æä¾›ä¸€ä¸ªå…¨é¢è€Œæ·±å…¥çš„LLMä½¿ç”¨æŒ‡å—ã€‚

### 1. LLMç”Ÿæ€ç³»ç»Ÿæ¦‚è¿°

å‘å±•å†ç¨‹ï¼š
ChatGPT æ˜¯ç”± OpenAI åœ¨ 2022 å¹´æ¨å‡ºçš„ï¼Œé¦–æ¬¡å…è®¸ç”¨æˆ·é€šè¿‡æ–‡æœ¬ç•Œé¢ä¸ LLM è¿›è¡Œäº¤äº’ï¼Œå¹¶è¿…é€Ÿåœ¨äº’è”ç½‘ä¸Šèµ°çº¢ã€‚â€œchpt it was developed by openai and deployed in 2022 so this was the first time that people could actually just kind of like talk to a large language model through a text interface and this went viralâ€ã€‚
å½“å‰ç”Ÿæ€ç³»ç»Ÿæ—¥ç›Šä¸°å¯Œï¼Œæ¶Œç°å‡ºè®¸å¤šç±»ä¼¼ ChatGPT çš„åº”ç”¨ï¼Œå°½ç®¡ ChatGPT ä»æ˜¯åŠŸèƒ½æœ€ä¸°å¯Œä¸”æœ€å—æ¬¢è¿çš„ã€‚
ä¸»è¦å‚ä¸è€…ï¼š
å¤§å‹ç§‘æŠ€å…¬å¸ï¼šGoogle (Gemini), Meta (co-pilot), Microsoft (co-pilot)ã€‚
åˆåˆ›å…¬å¸ï¼šAnthropic (Claud), xAI (Grok)ã€‚
å›½é™…å…¬å¸ï¼šDeepseek (ä¸­å›½), Mistral (æ³•å›½)ã€‚
æ¨¡å‹è¿½è¸ªä¸è¯„ä¼°ï¼š
Chatbot Arena:æä¾›ä¸åŒæ¨¡å‹çš„æ’åå’ŒELOè¯„åˆ†ã€‚â€œchatbot arena is one of them so here you can come to some ranking of different models and you can see sort of their strength or ELO scoreâ€ã€‚
Scale AI Leaderboard:å±•ç¤ºå„ç§è¯„ä¼°æŒ‡æ ‡ä¸‹æ¨¡å‹çš„è¡¨ç°ã€‚â€œseal Le leaderboard from scale and so here you can also see different kinds of eval and different kinds of models and how well they rank and you can also come here to see which models are currently performing the best on a wide variety of tasksâ€ã€‚


```mermaid
flowchart TD
    A[LLM Ecosystem] --> B[Development Timeline]
    B --> C[2022: ChatGPT Launch]
    B --> D[Current: Diverse Applications]
    A --> E[Major Players]
    E --> F[Tech Giants]
    F --> G[Google Gemini]
    F --> H[Meta Co-pilot]
    F --> I[Microsoft Co-pilot]
    E --> J[Startups]
    J --> K[Anthropic Claud]
    J --> L[xAI Grok]
    E --> M[International]
    M --> N[Deepseek China]
    M --> O[Mistral France]
    A --> P[Evaluation Metrics]
    P --> Q[Chatbot Arena]
    P --> R[Scale AI Leaderboard]
```

### 2. ä¸LLMçš„åŸºæœ¬äº¤äº’æ–¹å¼

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡LLMæ˜¯ä¸ªè®°å¿†åŠ›è¶…ç¾¤çš„å›¾ä¹¦ç®¡ç†å‘˜ğŸ“š
- **åŸºæœ¬èƒ½åŠ›**ï¼šå¤§è„‘å­˜å‚¨æµ·é‡ä¹¦ç±ï¼ˆé¢„è®­ç»ƒæ•°æ®ï¼‰
- **äº¤äº’é™åˆ¶** => æ¯æ¬¡å¯¹è¯åªèƒ½å¸¦1æœ¬ç¬”è®°æœ¬ï¼ˆä¸Šä¸‹æ–‡çª—å£ï¼‰

```mermaid
flowchart LR
    A[ç”¨æˆ·æé—®] --> B[æ‹†è§£æˆå•è¯ç§¯æœ¨]
    B --> C{æ˜¯å¦æ–°å¯¹è¯?}
    C -- æ˜¯ --> D[ä½¿ç”¨ç©ºç™½ç¬”è®°æœ¬]
    C -- å¦ --> E[ç»§ç»­å½“å‰ç¬”è®°]
    E --> F[æŸ¥æ‰¾è®°å¿†ä¹¦åº“]
    D --> F
    F --> G[ç»„åˆæ–°ç§¯æœ¨å›åº”]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style B fill:#C8E6C9,stroke:#43A047
    style F fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```
ç¤ºä¾‹ï¼šè¦æ±‚ LLM åˆ›ä½œä¸€é¦–å…³äº LLM ä½“éªŒçš„ä¿³å¥ã€‚
# "ä½ å¥½!" æ‹†è§£ä¸ºï¼š
tokens = ["ä½ ", "å¥½", "!"]
# è‹±æ–‡ "Hello!" æ‹†è§£ä¸ºï¼š
tokens = ["Hello", "!"]
TokenåŒ–ï¼š
ç”¨æˆ·è¾“å…¥å’Œæ¨¡å‹è¾“å‡ºéƒ½è¢«åˆ†è§£æˆç§°ä¸º â€œtokensâ€ çš„æ–‡æœ¬å—ã€‚â€œthis sequence of text is under the hood a token sequence onedimensional token sequenceâ€ã€‚
Tokenizer ç­‰å·¥å…·å¯ç”¨äºæŸ¥çœ‹æ–‡æœ¬çš„ token åºåˆ—ã€‚â€œwe can use an app like for example Tik tokenizer so making sure that GPT 40 is selected I can paste my text here and this is actually what the model sees Under the Hoodâ€ã€‚
Token æ•°é‡å½±å“æ¨¡å‹å¤„ç†é€Ÿåº¦å’Œæˆæœ¬ã€‚
ä¼šè¯æ ¼å¼ï¼š
LLM çš„ â€œè®°å¿†â€ é€šè¿‡ä¸Šä¸‹æ–‡çª—å£ï¼ˆcontext windowï¼‰å®ç°ï¼Œè¯¥çª—å£ä¿å­˜äº†å¯¹è¯ä¸­çš„ token åºåˆ—ã€‚â€œthe context window is kind of like this working memory of tokens and anything that is inside this context window is kind of like in the working memory of this conversation and is very directly accessible by the modelâ€ã€‚


å½“å¼€å¯æ–°çš„èŠå¤©æ—¶ï¼Œä¸Šä¸‹æ–‡çª—å£ä¼šè¢«æ¸…é™¤ï¼Œtoken è®¡æ•°é‡ç½®ã€‚â€œwhen I click new chat here that wipes the token window that resets the tokens to basically zero again and restarts the conversation from scratchâ€ã€‚

### 3. LLMçš„å†…éƒ¨è¿ä½œæœºåˆ¶

- è®­ç»ƒé˜¶æ®µï¼š
é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰ï¼š LLM é€šè¿‡å‹ç¼©äº’è”ç½‘ä¸Šçš„å¤§é‡æ–‡æœ¬æ•°æ®å­¦ä¹ ä¸–ç•ŒçŸ¥è¯†ã€‚â€œthe pre-training stage is kind of like taking all of Internet chopping it up into tokens and then compressing it into a single kind of like zip file but the zip file is not exact the zip file is lossy and probabilistic zip fileâ€ã€‚é¢„è®­ç»ƒæˆæœ¬é«˜æ˜‚ï¼Œå› æ­¤æ¨¡å‹çŸ¥è¯†å­˜åœ¨ 
â€œçŸ¥è¯†æˆªæ–­â€ï¼ˆknowledge cutoffï¼‰ã€‚

- åè®­ç»ƒé˜¶æ®µï¼š
åè®­ç»ƒï¼ˆPost-trainingï¼‰ï¼š LLM é€šè¿‡äººç±»å¯¹è¯æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ å¦‚ä½•ä»¥åŠ©æ‰‹çš„èº«ä»½å›åº”ç”¨æˆ·æŸ¥è¯¢ã€‚â€œpost-training Stage is really attaching a smiley face to this ZIP file because we don't want to generate internet documents we want this thing to take on the Persona of an assistant that responds to user queries and that's done in a process of post training where we swap out the data set for a data set of conversations that are built out by humansâ€ã€‚

- LLMçš„æœ¬è´¨ï¼š
LLM æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¤§å‹ â€œzip æ–‡ä»¶â€ï¼ŒåŒ…å«æ•°åäº¿ä¸ªå‚æ•°ï¼Œç”¨äºé¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ª tokenã€‚â€œwhat you are talking to to is a fully self-contained entity by default this language model think of it as a one tbte file on a dis secretly that represents one trillion parameters and their precise settings inside the neural network that's trying to give you the next token in the sequenceâ€ã€‚
é»˜è®¤æƒ…å†µä¸‹ï¼ŒLLM æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å®ä½“ï¼Œä¸å…·å¤‡è®¡ç®—å™¨ã€Python è§£é‡Šå™¨æˆ–äº’è”ç½‘æµè§ˆå™¨ç­‰å·¥å…·ã€‚â€œyou're talking to a zip file if you stream tokens to it it will respond with tokens back and this ZIP file has the knowledge from pre-training and it has the style and form from posttrainingâ€ã€‚

- LLMé€‚ç”¨åœºæ™¯ä¸¾ä¾‹
æŸ¥è¯¢å’–å•¡å› å«é‡ï¼Œç¡®è®¤è¯å“æˆåˆ†ã€‚â€œthis morning I asked Chachi the following how much caffeine is in one shot of Americana and I was curious because I was com... conversations of DayQuil and NyQuil these are very common meds...â€.

```mermaid
flowchart TD
    A[Pre-training] -->|Compress internet data| B[Knowledge Base]
    B -->|Billions of parameters| C[LLM Core]
    D[Post-training] -->|Human conversations| E[Assistant Personality]
    C --> F[Model Capabilities]
    E --> F
    F --> G[Token Prediction]
    F --> H[Query Response]
    style A fill:#B3E5FC,stroke:#039BE5
    style B fill:#C8E6C9,stroke:#43A047
    style D fill:#FFF9C4,stroke:#FDD835
    style E fill:#FFCCBC,stroke:#FF5722
```

### 4. LLMä½¿ç”¨æŠ€å·§

```mermaid
flowchart TD
    A[LLM Usage Tips] --> B[Start New Chat]
    A --> C[Select Appropriate Model]
    A --> D[Understand Free vs Paid]
    B --> E[Clear Context Window]
    B --> F[Improve Accuracy & Speed]
    C --> G[Size vs Cost Tradeoff]
    C --> H[Bigger Models = More Powerful]
    D --> I[Free Version Limitations]
    D --> J[Paid Version Benefits]
    style A fill:#B3E5FC,stroke:#039BE5
    style B fill:#C8E6C9,stroke:#43A047
    style C fill:#FFF9C4,stroke:#FDD835
    style D fill:#FFCCBC,stroke:#FF5722
```

- åŠæ—¶æ–°å»ºä¼šè¯ï¼š
å½“åˆ‡æ¢è¯é¢˜æ—¶ï¼Œå»ºè®®å¼€å¯æ–°çš„èŠå¤©ï¼Œä»¥æ¸…é™¤ä¸Šä¸‹æ–‡çª—å£ä¸­ä¸ç›¸å…³çš„ tokenï¼Œæé«˜æ¨¡å‹å‡†ç¡®æ€§å’Œé€Ÿåº¦ã€‚â€œanytime you are switching topic I encourage you to always start a new chat when you start a new chat as we talked about you are wiping the context window of tokens and resetting it back to zeroâ€ã€‚

- é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š
ä¸åŒçš„ LLM æä¾›å•† æä¾›ä¸åŒå®šä»·å±‚çº§çš„æ¨¡å‹ã€‚ å¤§å‹æ¨¡å‹åŠŸèƒ½æ›´å¼ºå¤§ï¼Œä½†æˆæœ¬æ›´é«˜ã€‚â€œbe mindful of the models that you're using typically with these companies the bigger models are more expensive to uh calculate and so therefore uh the companies charge more for the bigger models and so make those tradeoffs for yourself depending on your usage of llmsâ€ã€‚

- æ³¨æ„åŒºåˆ†å…è´¹å’Œä»˜è´¹ç‰ˆæœ¬çš„åŠŸèƒ½å·®å¼‚ï¼Œä»˜è´¹ç”¨æˆ·é€šå¸¸å¯ä»¥è®¿é—®æ›´å¼ºå¤§çš„æ¨¡å‹ã€‚â€œplus users get 80 messages every 3 hours for GPT 40 so that's the flagship biggest model that's currently available as of todayâ€ã€‚

### 5. æ€è€ƒå‹æ¨¡å‹(Thinking Models)

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡LLMæ˜¯ä¸ªè§£é¢˜é«˜æ‰‹ğŸ§ 
- **åŸºç¡€èƒ½åŠ›**ï¼šè®°å¿†é¢˜åº“ï¼ˆé¢„è®­ç»ƒï¼‰
- **å¼ºåŒ–è®­ç»ƒ** => å‚åŠ è§£é¢˜ç«èµ›ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰

```mermaid
flowchart TD
    A[åŸºç¡€æ¨¡å‹] --> B{æ˜¯å¦æ€è€ƒå‹?}
    B -- æ˜¯ --> C[å¼ºåŒ–å­¦ä¹ è®­ç»ƒ]
    B -- å¦ --> D[ç›´æ¥è¾“å‡ºç­”æ¡ˆ]
    C --> E[å°è¯•å¤šç§è§£é¢˜è·¯å¾„]
    E --> F{æœ€ä¼˜è§£?}
    F -- æ˜¯ --> G[è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ]
    F -- å¦ --> E
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```
```mermaid
sequenceDiagram
    ç”¨æˆ·->>+æ€è€ƒå‹æ¨¡å‹: æäº¤ä»£ç è°ƒè¯•è¯·æ±‚
    æ€è€ƒå‹æ¨¡å‹->>æ€è€ƒå‹æ¨¡å‹: åˆ†æä»£ç ç»“æ„ï¼ˆ10ç§’ï¼‰
    æ€è€ƒå‹æ¨¡å‹->>æ€è€ƒå‹æ¨¡å‹: å°è¯•ä¿®å¤æ–¹æ¡ˆAï¼ˆ20ç§’ï¼‰
    æ€è€ƒå‹æ¨¡å‹->>æ€è€ƒå‹æ¨¡å‹: éªŒè¯æ–¹æ¡ˆA â†’ å¤±è´¥
    æ€è€ƒå‹æ¨¡å‹->>æ€è€ƒå‹æ¨¡å‹: å°è¯•ä¿®å¤æ–¹æ¡ˆBï¼ˆ20ç§’ï¼‰
    æ€è€ƒå‹æ¨¡å‹->>æ€è€ƒå‹æ¨¡å‹: éªŒè¯æ–¹æ¡ˆB â†’ æˆåŠŸ
    æ€è€ƒå‹æ¨¡å‹-->>-ç”¨æˆ·: è¿”å›æ­£ç¡®è§£å†³æ–¹æ¡ˆ

```

- è®­ç»ƒè¿‡ç¨‹ï¼š
æ€è€ƒå‹æ¨¡å‹é€šè¿‡å¼ºåŒ–å­¦ä¹ è¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ è§£å†³é—®é¢˜çš„ç­–ç•¥ã€‚â€œwe saw in the previous video that there are multiple stages of training pre-training goes to supervised fine tuning goes to reinforcement learning and reinforcement learning is where the model gets to practiceâ€ã€‚ å¼ºåŒ–å­¦ä¹ è®©æ¨¡å‹èƒ½å¤Ÿå°è¯•å„ç§æ–¹æ³•ï¼Œæ‰¾åˆ°æœ€ä½³çš„æ€è€ƒè¿‡ç¨‹ã€‚

- ç‰¹ç‚¹ï¼š
ç›¸æ¯”æ™®é€šæ¨¡å‹ï¼Œæ€è€ƒå‹æ¨¡å‹ä¼šè¿›è¡Œæ›´å¤šçš„ â€œæ€è€ƒâ€ï¼Œåœ¨è§£å†³æ•°å­¦ã€ä»£ç ç­‰å¤æ‚é—®é¢˜æ—¶è¡¨ç°æ›´ä½³ã€‚â€œthe model will do a lot more thinking and what you can expect is that you will get higher accuracies especially on problems that are for example math code and things that require a lot of thinkingâ€ã€‚
æ€è€ƒè¿‡ç¨‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œæ¨¡å‹ä¼šè¾“å‡ºå¤§é‡çš„ tokenã€‚
- æ¡ˆä¾‹åˆ†æï¼š
ä½¿ç”¨æ€è€ƒå‹æ¨¡å‹è§£å†³ä»£ç è°ƒè¯•é—®é¢˜ï¼Œç›¸æ¯”æ™®é€šæ¨¡å‹èƒ½æ›´å¿«æ‰¾åˆ°è§£å†³æ–¹æ¡ˆã€‚â€œwhen I gave the same model the same prompt to 01 Pro which is the best at reasoning model and you have to pay $200 per month for this one then the exact same prompt it went off and it thought for 1 minute and it went through a sequence of thoughts...then it actually came to get came back with the correct solutionâ€ã€‚

### 6. å·¥å…·ä½¿ç”¨(Tool Use)

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡LLMæ˜¯ä¸ªä¿¡æ¯ä¾¦æ¢ğŸ•µï¸
- **åŸºæœ¬èƒ½åŠ›**ï¼šè®°å¿†æ¡£æ¡ˆåº“ï¼ˆé¢„è®­ç»ƒæ•°æ®ï¼‰
- **æœç´¢å·¥å…·** => å®æ—¶è·å–æœ€æ–°æƒ…æŠ¥ï¼ˆçªç ´çŸ¥è¯†æˆªæ­¢æ—¥æœŸï¼‰

```mermaid
flowchart TD
    A[ç”¨æˆ·æé—®] --> B{éœ€è¦æœ€æ–°ä¿¡æ¯?}
    B -- æ˜¯ --> C[å‘é€æœç´¢æŒ‡ä»¤]
    B -- å¦ --> D[ç›´æ¥å›ç­”]
    C --> E[æ‰§è¡Œç½‘ç»œæœç´¢]
    E --> F[è·å–ç½‘é¡µå†…å®¹]
    F --> G[æ•´åˆä¿¡æ¯ç”Ÿæˆå›ç­”]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```



- äº’è”ç½‘æœç´¢ï¼š
LLM å¯ä»¥é€šè¿‡äº’è”ç½‘æœç´¢è·å–æœ€æ–°ä¿¡æ¯ï¼Œè§£å†³çŸ¥è¯†æˆªæ–­é—®é¢˜ã€‚â€œwe introduce a mechanism for for the model to emit a special token that is some kind of a searchy internet token and when the model emits the searchd internet token the Chach PT application or whatever llm application it is you're using will stop sampling from the model and it will take the query that the model model gave it goes off it does a searchâ€ã€‚


æ¨¡å‹å‘å‡ºç‰¹å®šçš„ tokenï¼ŒæŒ‡ç¤ºåº”ç”¨ç¨‹åºè¿›è¡Œäº’è”ç½‘æœç´¢ï¼Œå¹¶å°†æœç´¢ç»“æœæ’å…¥åˆ°ä¸Šä¸‹æ–‡çª—å£ä¸­ã€‚â€œso now you have this internet search tool that itself can also contribute tokens into our context window and in this case it would be like lots of internet web pages and maybe there's 10 of them and maybe it just puts it all together and this could be thousands of tokens coming from these web pagesâ€ã€‚



```mermaid
sequenceDiagram
    ç”¨æˆ·->>+LLM: æŸ¥è¯¢æœ€æ–°ç§‘æŠ€æ–°é—»
    LLM->>LLM: æ£€æŸ¥çŸ¥è¯†åº“ â†’ è¿‡æœŸ
    LLM->>æœç´¢å¼•æ“: å‘é€æœç´¢è¯·æ±‚
    æœç´¢å¼•æ“-->>LLM: è¿”å›æ–°é—»ç½‘é¡µ
    LLM->>LLM: æå–å…³é”®ä¿¡æ¯
    LLM-->>-ç”¨æˆ·: è¿”å›æœ€æ–°æ–°é—»æ‘˜è¦
```

### 6. å·¥å…·ä½¿ç”¨(Tool Use)

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡LLMæ˜¯ä¸ªä¿¡æ¯ä¾¦æ¢ğŸ•µï¸
- **åŸºæœ¬èƒ½åŠ›**ï¼šè®°å¿†æ¡£æ¡ˆåº“ï¼ˆé¢„è®­ç»ƒæ•°æ®ï¼‰
- **æœç´¢å·¥å…·** => å®æ—¶è·å–æœ€æ–°æƒ…æŠ¥ï¼ˆçªç ´çŸ¥è¯†æˆªæ­¢æ—¥æœŸï¼‰

```mermaid
flowchart TD
    A[ç”¨æˆ·æé—®] --> B{éœ€è¦æœ€æ–°ä¿¡æ¯?}
    B -- æ˜¯ --> C[å‘é€æœç´¢æŒ‡ä»¤]
    B -- å¦ --> D[ç›´æ¥å›ç­”]
    C --> E[æ‰§è¡Œç½‘ç»œæœç´¢]
    E --> F[è·å–ç½‘é¡µå†…å®¹]
    F --> G[æ•´åˆä¿¡æ¯ç”Ÿæˆå›ç­”]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```

æœç´¢æœºåˆ¶è¯¦è§£ ï¼š

1. è§¦å‘ä¿¡å· ï¼ˆä¾¦æ¢çš„æœç´¢ä»¤ï¼‰ï¼š
   
   - ç‰¹æ®Štokenï¼š [SEARCH]
   - ç¤ºä¾‹ï¼š
     ```python
     # æ¨¡å‹è¾“å‡º
     tokens = ["æ ¹æ®", "æœ€æ–°", "æ•°æ®", "[SEARCH]"]
     ```
2. ä¿¡æ¯æ•´åˆ ï¼ˆä¾¦æ¢çš„æŠ¥å‘Šï¼‰ï¼š
   
   ```ç¤ºä¾‹
   ç”¨æˆ·é—®ï¼š"2023å¹´è¯ºè´å°”ç»æµå­¦å¥–å¾—ä¸»æ˜¯è°ï¼Ÿ"
   
   LLMå¤„ç†è¿‡ç¨‹ï¼š
   1. æ£€æŸ¥è®°å¿† â†’ çŸ¥è¯†æˆªæ­¢2023.10
   2. å‘é€æœç´¢æŒ‡ä»¤ â†’ è·å–æœ€æ–°ç½‘é¡µ
   3. æå–å…³é”®ä¿¡æ¯ â†’ ç»„ç»‡æˆè‡ªç„¶è¯­è¨€
   4. æ ‡æ³¨æ¥æºï¼šã€Œæ ¹æ®è¯ºè´å°”å®˜ç½‘2023å¹´æ›´æ–°...ã€
   ```
3. æ€§èƒ½å¯¹æ¯” ï¼š
    ä¿¡æ¯ç±»å‹ å¤„ç†æ–¹å¼ å“åº”æ—¶é—´ å‡†ç¡®æ€§ æˆªæ­¢æ—¥æœŸå‰
   
   ç›´æ¥å›ç­”
   
   1-2ç§’
   
   100% æœ€æ–°ä¿¡æ¯
   
   ç½‘ç»œæœç´¢
   
   3-5ç§’
   
   95%+

- é€‚ç”¨åœºæ™¯ï¼š
æŸ¥è¯¢æœ€æ–°ä¿¡æ¯ï¼Œå¦‚ White Lotus ç¬¬ä¸‰å­£çš„æ’­å‡ºæ—¶é—´ã€‚â€œI was curious when the episode two was coming outâ€ã€‚
ç ”ç©¶ç‰¹å®šä¸»é¢˜ï¼Œå¦‚ Brian Johnson çš„ç‰™è†æˆåˆ†ã€‚â€œthis can potentially change over time and then I saw a bunch of stuff on Twitter about a USA ID and I wanted to know kind of like what's the dealâ€ã€‚

### 7. æ·±åº¦ç ”ç©¶(Deep Research)

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡LLMæ˜¯ä¸ªç ”ç©¶åŠ©ç†ğŸ‘¨â€ğŸ”¬
- **åŸºæœ¬èƒ½åŠ›**ï¼šå¿«é€Ÿé˜…è¯»ï¼ˆä¿¡æ¯å¤„ç†ï¼‰
- **ç ”ç©¶å·¥å…·** => æ–‡çŒ®æ£€ç´¢+æ·±åº¦åˆ†æï¼ˆçªç ´äººç±»ç ”ç©¶é€Ÿåº¦ï¼‰

```mermaid
flowchart TD
    A[ç ”ç©¶ä¸»é¢˜] --> B{éœ€è¦æ·±åº¦ç ”ç©¶?}
    B -- æ˜¯ --> C[å¯åŠ¨ç ”ç©¶æ¨¡å¼]
    B -- å¦ --> D[ç›´æ¥å›ç­”]
    C --> E[æ–‡çŒ®æ£€ç´¢]
    E --> F[æ·±åº¦åˆ†æ]
    F --> G[ç”Ÿæˆç ”ç©¶æŠ¥å‘Š]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```

- è¡Œä¸šåº”ç”¨ï¼š
Perplexity å’Œ Grok ä¹Ÿæä¾›äº†ç±»ä¼¼çš„æ·±åº¦ç ”ç©¶åŠŸèƒ½ã€‚â€œperplexity when you go to the model drop down has something called Deep research and so you can issue the same queries here and we can give this to perplexity and then grock as well has something called Deep search instead of deep researchâ€ã€‚

ç”¨æˆ·é—®ï¼š"è¯·åˆ†æé‡å­è®¡ç®—çš„æœ€æ–°è¿›å±•"

LLMå¤„ç†è¿‡ç¨‹ï¼š
1. ç¡®å®šç ”ç©¶èŒƒå›´ â†’ é‡å­è®¡ç®—
2. æ£€ç´¢æœ€æ–°æ–‡çŒ® â†’ è·å–ç›¸å…³è®ºæ–‡
3. æ·±åº¦åˆ†æå†…å®¹ â†’ æå–å…³é”®ä¿¡æ¯
4. ç»„ç»‡ç ”ç©¶æŠ¥å‘Š â†’ ç”Ÿæˆç»“æ„åŒ–è¾“å‡º


```mermaid
sequenceDiagram
    ç”¨æˆ·->>+LLM: è¯·æ±‚æ·±åº¦ç ”ç©¶
    LLM->>LLM: ç¡®å®šç ”ç©¶èŒƒå›´
    LLM->>æ–‡çŒ®åº“: æ£€ç´¢ç›¸å…³æ–‡çŒ®
    æ–‡çŒ®åº“-->>LLM: è¿”å›ç ”ç©¶èµ„æ–™
    LLM->>LLM: æ·±åº¦åˆ†æå†…å®¹
    LLM-->>-ç”¨æˆ·: ç”Ÿæˆç ”ç©¶æŠ¥å‘Š
```

- æ³¨æ„äº‹é¡¹ï¼š
æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¯èƒ½å­˜åœ¨ â€œå¹»è§‰â€ï¼Œç”¨æˆ·éœ€è¦ä»”ç»†æ ¸å¯¹ä¿¡æ¯æ¥æºã€‚â€œeven though it is doing research and it's pulling in there are no guarantees that there are no hallucinations hereâ€ã€‚
å¼•ç”¨æ–‡çŒ®åº”è¯¥è¦è‡ªè¡Œç¡®è®¤ã€‚

### 8. æ–‡æ¡£ä¸Šä¼ ä¸åˆ†æ

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡LLMæ˜¯ä¸ªæ™ºèƒ½æ‰«æä»ªğŸ“„
- **åŸºæœ¬èƒ½åŠ›**ï¼šå¿«é€Ÿé˜…è¯»ï¼ˆä¿¡æ¯å¤„ç†ï¼‰
- **æ–‡æ¡£å·¥å…·** => å°†çº¸è´¨æ–‡æ¡£æ•°å­—åŒ–ï¼ˆçªç ´ä¼ ç»Ÿé˜…è¯»æ–¹å¼ï¼‰

```mermaid
flowchart TD
    A[ç”¨æˆ·] --> B{éœ€è¦åˆ†ææ–‡æ¡£?}
    B -- æ˜¯ --> C[ä¸Šä¼ æ–‡æ¡£]
    B -- å¦ --> D[ç›´æ¥å¯¹è¯]
    C --> E[æ–‡æ¡£è§£æ]
    E --> F[å†…å®¹æå–]
    F --> G[ç”Ÿæˆæ‘˜è¦]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```
ç”¨æˆ·ä¸Šä¼ ï¼š"2023å¹´å…¬å¸è´¢æŠ¥.pdf"

LLMå¤„ç†è¿‡ç¨‹ï¼š
1. æ–‡æ¡£è§£æ â†’ æå–æ–‡æœ¬å†…å®¹
2. å…³é”®ä¿¡æ¯æå– â†’ è´¢åŠ¡æ•°æ®
3. ç”Ÿæˆæ‘˜è¦ â†’ æ€»ç»“ä¸»è¦è´¢åŠ¡æŒ‡æ ‡
4. æ”¯æŒé—®ç­” â†’ å›ç­”å…·ä½“é—®é¢˜

```mermaid
sequenceDiagram
    ç”¨æˆ·->>+LLM: ä¸Šä¼ å­¦æœ¯è®ºæ–‡
    LLM->>LLM: è§£æPDFæ ¼å¼
    LLM->>LLM: æå–å…³é”®ä¿¡æ¯
    LLM-->>-ç”¨æˆ·: ç”Ÿæˆè®ºæ–‡æ‘˜è¦
```

- é€‚ç”¨åœºæ™¯ï¼š
* é˜…è¯»å­¦æœ¯è®ºæ–‡ï¼Œå¿«é€Ÿè·å–è®ºæ–‡æ‘˜è¦å’Œå…³é”®ä¿¡æ¯ã€‚â€œso typically when I start reading papers together with any of these llms I just ask for can you uh give me a summary uh summary of this paper let's see what cloud 3.7 saysâ€ã€‚
* é˜…è¯»ä¹¦ç±ï¼ŒåŠ æ·±å¯¹å†…å®¹çš„ç†è§£ã€‚â€œway I read books now as an example is uh you basically pull up the book and you have to get uh access to like the raw content of that informationâ€ã€‚


### 9. ä»£ç è§£é‡Šå™¨(Python Interpreter)

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡LLMæ˜¯ä¸ªç¼–ç¨‹åŠ©æ‰‹ğŸ’»
- **åŸºæœ¬èƒ½åŠ›**ï¼šç¼–å†™ä»£ç ï¼ˆä»£ç ç”Ÿæˆï¼‰
- **è§£é‡Šå™¨å·¥å…·** => æ‰§è¡Œä»£ç å¹¶è¿”å›ç»“æœï¼ˆçªç ´çº¯æ–‡æœ¬é™åˆ¶ï¼‰

```mermaid
flowchart TD
    A[ç”¨æˆ·æé—®] --> B{éœ€è¦è®¡ç®—?}
    B -- æ˜¯ --> C[ç”ŸæˆPythonä»£ç ]
    B -- å¦ --> D[ç›´æ¥å›ç­”]
    C --> E[æ‰§è¡Œä»£ç ]
    E --> F[è·å–ç»“æœ]
    F --> G[è¿”å›è®¡ç®—ç»“æœ]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```

LLM å¯ä»¥ä½¿ç”¨ Python è§£é‡Šå™¨æ‰§è¡Œä»£ç ï¼Œè§£å†³å¤æ‚çš„æ•°å­¦è®¡ç®—å’Œæ•°æ®åˆ†æé—®é¢˜ã€‚â€œinstead of the llm giving you an answer directly it has the ability now to write a computer program and to emit special tokens that the chpt application recognizes as hey this is not for the human this is uh basically saying that whatever I output it here uh is actually a computer program please go off and run it and give me the result of running that computer programâ€ã€‚

ç”¨æˆ·é—®ï¼š"è®¡ç®—1åˆ°100çš„å’Œ"

LLMå¤„ç†è¿‡ç¨‹ï¼š
1. ç”ŸæˆPythonä»£ç  â†’ sum(range(1, 101))
2. æ‰§è¡Œä»£ç  â†’ 5050
3. è¿”å›ç»“æœ â†’ "1åˆ°100çš„å’Œæ˜¯5050"

```mermaid
sequenceDiagram
    ç”¨æˆ·->>+LLM: è¯·æ±‚æ•°æ®åˆ†æ
    LLM->>LLM: ç”ŸæˆPythonä»£ç 
    LLM->>Pythonè§£é‡Šå™¨: æ‰§è¡Œä»£ç 
    Pythonè§£é‡Šå™¨-->>LLM: è¿”å›ç»“æœ
    LLM-->>-ç”¨æˆ·: è¿”å›åˆ†ææŠ¥å‘Š
```


é«˜çº§æ•°æ®åˆ†æï¼š
ChatGPT ç»“åˆé«˜çº§æ•°æ®åˆ†æåŠŸèƒ½ï¼Œå¯ä»¥å……å½“åˆçº§æ•°æ®åˆ†æå¸ˆï¼Œè¿›è¡Œæ•°æ®å¯è§†åŒ–å’Œè¶‹åŠ¿åˆ†æã€‚â€œthis is where this gets powerful Chachi PT goes off and writes a program that plots the data over here so it cre a little figure for us and it uh sort of uh ran it and showed it to usâ€ã€‚


**æ³¨æ„ï¼š**éœ€è¦ä»”ç»†å®¡æŸ¥ä»£ç å’Œåˆ†æç»“æœï¼Œé¿å…æ¨¡å‹äº§ç”Ÿé”™è¯¯ã€‚â€œyou have to be careful and scrutinize it and make sure that you are really watching very closely because your Junior analyst is a little bit uh absent minded and uh not quite right all the timeâ€ã€‚

### 10. äº‘ç«¯ Artifacts(Cloud Artifacts)

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡Claudeæ˜¯ä¸ªå…¨æ ˆå¼€å‘è€…ğŸ‘¨â€ğŸ’»
- **åŸºæœ¬èƒ½åŠ›**ï¼šç¼–å†™ä»£ç ï¼ˆä»£ç ç”Ÿæˆï¼‰
- **Artifactså·¥å…·** => ç”Ÿæˆå¯äº¤äº’Webåº”ç”¨ï¼ˆçªç ´çº¯æ–‡æœ¬é™åˆ¶ï¼‰

```mermaid
flowchart TD
    A[ç”¨æˆ·éœ€æ±‚] --> B{éœ€è¦Webåº”ç”¨?}
    B -- æ˜¯ --> C[ç”ŸæˆReactä»£ç ]
    B -- å¦ --> D[ç›´æ¥å›ç­”]
    C --> E[æ„å»ºç»„ä»¶]
    E --> F[éƒ¨ç½²åº”ç”¨]
    F --> G[è¿”å›åº”ç”¨é“¾æ¥]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```

ç”¨æˆ·éœ€æ±‚ï¼š"åˆ›å»ºä¸€ä¸ªFlashcardåº”ç”¨"

Claudeå¤„ç†è¿‡ç¨‹ï¼š
1. ç”ŸæˆReactä»£ç  â†’ åˆ›å»ºç»„ä»¶
2. æ„å»ºåº”ç”¨ â†’ éƒ¨ç½²åˆ°äº‘ç«¯
3. è¿”å›é“¾æ¥ â†’ ç”¨æˆ·å¯ç›´æ¥è®¿é—®

```mermaid
sequenceDiagram
    ç”¨æˆ·->>+Claude: è¯·æ±‚Flashcardåº”ç”¨
    Claude->>Claude: ç”ŸæˆReactä»£ç 
    Claude->>äº‘ç«¯: éƒ¨ç½²åº”ç”¨
    äº‘ç«¯-->>Claude: è¿”å›åº”ç”¨é“¾æ¥
    Claude-->>-ç”¨æˆ·: è¿”å›åº”ç”¨åœ°å€
```


æ€ç»´å¯è§†åŒ–
ç”¨å›¾è¡¨åˆ†ææ–‡æœ¬å†…å®¹ï¼ŒåŠ æ·±ç†è§£ã€‚
â€œI'm attaching chapter 3 and book one please create a conceptual diagram of this chapterâ€ã€‚

### 11. ä»£ç è¾…åŠ©å¼€å‘

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡Cursoræ˜¯ä¸ªä»£ç ç®¡å®¶ğŸ¤–
- **åŸºæœ¬èƒ½åŠ›**ï¼šæ‰§è¡Œå‘½ä»¤ï¼ˆå‘½ä»¤å“åº”ï¼‰
- **Vibe Coding** => è‡ªåŠ¨åŒ–ä»£ç å¼€å‘ï¼ˆçªç ´æ‰‹åŠ¨ç¼–ç é™åˆ¶ï¼‰

```mermaid
flowchart TD
    A[å¼€å‘è€…] --> B{éœ€è¦ä»£ç ä¿®æ”¹?}
    B -- æ˜¯ --> C[å‘å‡ºæŒ‡ä»¤]
    B -- å¦ --> D[æ‰‹åŠ¨ç¼–ç ]
    C --> E[åˆ†æéœ€æ±‚]
    E --> F[ä¿®æ”¹ä»£ç ]
    F --> G[è¿”å›ç»“æœ]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```
å¼€å‘è€…æŒ‡ä»¤ï¼š"å°†æ‰€æœ‰æŒ‰é’®é¢œè‰²æ”¹ä¸ºè“è‰²"

Cursorå¤„ç†è¿‡ç¨‹ï¼š
1. åˆ†æä»£ç  â†’ å®šä½æ‰€æœ‰æŒ‰é’®
2. ä¿®æ”¹ä»£ç  â†’ æ›´æ–°æ ·å¼
3. è¿”å›ç»“æœ â†’ æ˜¾ç¤ºä¿®æ”¹åçš„ä»£ç 


```mermaid
sequenceDiagram
    å¼€å‘è€…->>+Cursor: è¯·æ±‚ä¿®æ”¹æŒ‰é’®æ ·å¼
    Cursor->>Cursor: åˆ†æä»£ç ç»“æ„
    Cursor->>Cursor: å®šä½æ‰€æœ‰æŒ‰é’®
    Cursor->>Cursor: æ›´æ–°æ ·å¼ä»£ç 
    Cursor-->>-å¼€å‘è€…: è¿”å›ä¿®æ”¹åçš„ä»£ç 
```
### 12. å¤šæ¨¡æ€äº¤äº’

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡LLMæ˜¯ä¸ªè¯­éŸ³åŠ©æ‰‹ğŸ™ï¸
- **åŸºæœ¬èƒ½åŠ›**ï¼šæ–‡æœ¬äº¤äº’ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰
- **è¯­éŸ³æ¨¡å¼** => ç›´æ¥éŸ³é¢‘è¾“å…¥è¾“å‡ºï¼ˆçªç ´æ–‡æœ¬é™åˆ¶ï¼‰

```mermaid
flowchart TD
    A[ç”¨æˆ·] --> B{ä½¿ç”¨è¯­éŸ³?}
    B -- æ˜¯ --> C[è¯­éŸ³è¾“å…¥]
    B -- å¦ --> D[æ–‡æœ¬è¾“å…¥]
    C --> E[éŸ³é¢‘è½¬æ–‡æœ¬]
    E --> F[å¤„ç†è¯·æ±‚]
    F --> G[æ–‡æœ¬è½¬éŸ³é¢‘]
    G --> H[è¯­éŸ³è¾“å‡º]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```
è¯­éŸ³è¾“å…¥
â€œwhat I use all the time on my MacBook is I basically fall back on some of these apps that um allow you that functionality but it's not specific to chat GPT it is a systemwide functionality of taking your audio and transcribing it into textâ€ã€‚
é€šè¿‡è¯­éŸ³æ–¹å¼èƒ½å¤Ÿæ›´å¿«é€Ÿå®ŒæˆæŒ‡ä»¤è¾“å…¥ï¼Œæ›´è‡ªç„¶åœ°äº¤äº’ã€‚

é«˜çº§è¯­éŸ³æ¨¡å¼(Advanced Voice Mode)

é«˜çº§è¯­éŸ³æ¨¡å¼(True audio)æ”¯æŒçœŸæ­£æ„ä¹‰ä¸Šçš„éŸ³è§†é¢‘è¾“å…¥è¾“å‡ºã€‚
â€œAdvanced voice mode is referring to True audio what that means is that the voice is handled natively inside the language model the model can understand audio chunks and predict audio chunks so it can hear and speak directly in audio there's no text involve d whatsoeverâ€ã€‚

Grokå¯ä»¥å¼€å¯æ— æ‹˜æŸæ¨¡å¼å°½æƒ…å¯¹è¯ï¼Œå¹¶æä¾›APPç«¯æ”¯æŒã€‚â€œgrock will just uh do stuff you know grock will grock will go there so if you prefer lot of entertainment I do think that the grock app is better set up for thatâ€ã€‚
æ ¹æ®æ–‡æœ¬å†…å®¹ç”Ÿæˆå®šåˆ¶podcast

notebook LMå¯ä»¥å°†æ–‡æœ¬è½¬æ¢æˆéŸ³é¢‘ã€‚
â€œon the right they have this uh Deep dive podcast so there's a generate button you can press it and wait like a few minutes and it will generate a custom podcast on whatever sources of information you put in hereâ€ã€‚

### 13. å›¾åƒæ¨¡å¼

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡LLMæ˜¯ä¸ªå›¾åƒå¤„ç†ä¸“å®¶ğŸ–¼ï¸
- **åŸºæœ¬èƒ½åŠ›**ï¼šæ–‡æœ¬ç†è§£ï¼ˆä¼ ç»Ÿæ¨¡å¼ï¼‰
- **å›¾åƒæ¨¡å¼** => è§†è§‰ä¿¡æ¯å¤„ç†ï¼ˆçªç ´çº¯æ–‡æœ¬é™åˆ¶ï¼‰

```mermaid
flowchart TD
    A[ç”¨æˆ·] --> B{ä½¿ç”¨å›¾åƒ?}
    B -- æ˜¯ --> C[å›¾åƒè¾“å…¥]
    B -- å¦ --> D[æ–‡æœ¬è¾“å…¥]
    C --> E[å›¾åƒtokenåŒ–]
    E --> F[è§†è§‰ä¿¡æ¯å¤„ç†]
    F --> G[ç”Ÿæˆå“åº”]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```

å¤šæ¨¡æ€å¤§æ¨¡å‹
å¯¹å›¾åƒtokenåŒ–ï¼Œå¹¶åº”ç”¨å’Œæ–‡æœ¬ä¸€æ ·çš„æ•°æ®æ¨¡å‹æ–¹å¼ã€‚
â€œyou can re-represent images in tokens and we can represent images as token streams and we can get language models to model them in the same way as we've modeled text and audio beforeâ€ã€‚
å›¾åƒè¾“å…¥
ä¸Šä¼ å›¾ç‰‡ï¼Œè¿›è¡Œåˆ†æï¼Œå¹¶ä¸”æä¾›ç†è§£å›¾ç‰‡çš„åŠŸèƒ½ã€‚
â€œif you go to your favorite chasht or other llm app you can upload images usually and ask questions of themâ€ã€‚
å›¾ç‰‡è¾“å‡º
ä½¿ç”¨å›¾ç‰‡ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚DALL-E 3ï¼Œé€šè¿‡æ–‡æœ¬ç”Ÿæˆå›¾ç‰‡ã€‚
â€œopen AI offering of this is called DOI and we're on the third version and it can generate really beautiful images on basically given arbitrary promptsâ€ã€‚
è§†é¢‘èƒ½åŠ›
GPTå…·å¤‡è§†é¢‘ç†è§£èƒ½åŠ›ã€‚

```mermaid
sequenceDiagram
    ç”¨æˆ·->>+LLM: ä¸Šä¼ äº§å“è®¾è®¡å›¾
    LLM->>LLM: å›¾åƒtokenåŒ–
    LLM->>LLM: è§†è§‰ç‰¹å¾æå–
    LLM->>LLM: ç”Ÿæˆè®¾è®¡å»ºè®®
    LLM-->>-ç”¨æˆ·: è¿”å›ä¼˜åŒ–æ–¹æ¡ˆ
```

### 14. è´¨é‡åŠŸèƒ½

**æ ¸å¿ƒç±»æ¯”**ï¼šæƒ³è±¡LLMæ˜¯ä¸ªç§äººåŠ©ç†ğŸ“
- **åŸºæœ¬èƒ½åŠ›**ï¼šå³æ—¶å“åº”ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰
- **è®°å¿†åŠŸèƒ½** => ä¸ªæ€§åŒ–æœåŠ¡ï¼ˆçªç ´å•æ¬¡å¯¹è¯é™åˆ¶ï¼‰

```mermaid
flowchart TD
    A[ç”¨æˆ·] --> B{éœ€è¦è®°å¿†?}
    B -- æ˜¯ --> C[å­˜å‚¨åå¥½]
    B -- å¦ --> D[ç›´æ¥å“åº”]
    C --> E[ä¿å­˜ä¿¡æ¯]
    E --> F[åç»­è°ƒç”¨]
    F --> G[ä¸ªæ€§åŒ–å“åº”]
    
    style A fill:#B3E5FC,stroke:#039BE5
    style C fill:#C8E6C9,stroke:#43A047
    style E fill:#FFF9C4,stroke:#FDD835
    style G fill:#FFCCBC,stroke:#FF5722
```


è®°å¿†åŠŸèƒ½
è®©GPTè®°ä½ä½ çš„ä¸€äº›å–œå¥½ï¼Œä»è€Œè¿›è¡Œä¸ªæ€§åŒ–åé¦ˆã€‚
â€œchat GPT does have an ability to save information from chat to chat but but it has to be invoked so sometimes chat GPT will trigger it automatically but sometimes you have to ask for it so basically say something along the lines of uh can you please remember this or like remember my preference or whatever something like thatâ€ã€‚

```mermaid
sequenceDiagram
    ç”¨æˆ·->>+LLM: è®¾ç½®åå¥½ï¼š"å–œæ¬¢è¯¦ç»†è§£é‡Š"
    LLM->>LLM: è¯†åˆ«è®°å¿†æŒ‡ä»¤
    LLM->>LLM: å­˜å‚¨ç”¨æˆ·åå¥½
    ç”¨æˆ·->>+LLM: åç»­æé—®
    LLM->>LLM: è°ƒç”¨è®°å¿†
    LLM-->>-ç”¨æˆ·: æä¾›è¯¦ç»†è§£é‡Š
```

è‡ªå®šä¹‰æŒ‡ä»¤
è‡ªå®šä¹‰æŒ‡ä»¤å¯ä»¥è®© GPT æ›´åŠ ç¬¦åˆä½ çš„ä¸ªæ€§ã€‚
â€œyou can come to settings um customize chpt and you see here it says what traes should chpt have and I just kind of like told it just don't be like an HR business partner just talk to me normally and also just give me I just lot explanations educations insights Etc so be educational whenever you canâ€ã€‚
è‡ªå®šä¹‰GPTs
åˆ©ç”¨Few-shotè®©GPTå®Œæˆç‰¹å®šä»»åŠ¡
â€œIf there's a certain prompt that you keep reusing then instead of reusing that prompt and copy pasting it over and over again just create a custom chat custom GPT save that prompt a single time and then what's changing per sort of use of it is the different sentenceâ€ã€‚

## ç»“è®º
å¤§å‹è¯­è¨€æ¨¡å‹æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†å¼ºå¤§çš„å·¥å…·æ¥è§£å†³å„ç§é—®é¢˜ã€‚ é€šè¿‡äº†è§£ LLM çš„å·¥ä½œåŸç†å’Œå„ç§åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å®ƒä»¬æ¥æé«˜å·¥ä½œæ•ˆç‡å’Œåˆ›é€ åŠ›ã€‚







